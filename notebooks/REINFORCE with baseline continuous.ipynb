{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d63649e",
   "metadata": {},
   "source": [
    "# REINFORCE with baseline for continuous action space\n",
    "\n",
    "This is an implementation of REINFORCE with baseline with modifications to deal with bounded continuous action spaces. See Sutton & Barto, Section 13.7.\n",
    "\n",
    "We make the following changes to the previous notebook:\n",
    "\n",
    "* Separate network for policy and value function\n",
    "  \n",
    "* Adjust policy network to output means and standard deviations of a normal distribution\n",
    "\n",
    "* Use of truncated normal distribution for bounded action space\n",
    "\n",
    "* Use of parameters given in the PPO paper, don't do hyper-parameter search anymore\n",
    "\n",
    "* Set up training and evaluation on the Mujoco environments used in the PPO paper, to be used as a comparison for our PPO implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39332cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torchrl.modules import TruncatedNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9192a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy needs to be modified to return a mean tensor and a standard deviations tensor\n",
    "# - use logits as mean\n",
    "# - standard deviations separate\n",
    "# - no dependencies between dimensions of multi-variate normal distribution\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden1, n_hidden2, n_out):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_in, n_hidden1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden1, n_hidden2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden2, n_out),\n",
    "        )\n",
    "        # self.sm = nn.Softmax(dim=0) # probabilities to choose an action\n",
    "        # self.lsm = nn.LogSoftmax(dim=0) # for log probabilities used in the gradient for REINFORCE\n",
    "\n",
    "        # From PPO, page 6: \n",
    "        # \"To represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\n",
    "        # and tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\n",
    "        # deviations, following [Sch+15b; Dua+16].\"\n",
    "        # (See [Sch+15b] (Trust Region Policy Optimization) on page 15.)\n",
    "        # Initialization of the standard deviations is not fully clear here, \n",
    "        # but to initialize them to 1 seems a reasonable first guess.\n",
    "        self.log_stddevs = nn.Parameter(torch.zeros(n_out))\n",
    "    \n",
    "    def forward(self, x): \n",
    "        logits = self.net(x)\n",
    "        # probs = self.sm(logits)\n",
    "        # scores = self.lsm(logits)\n",
    "        # return probs, scores, logits # we include the logits in the output for use in the state-value function\n",
    "        stddevs = torch.exp(self.log_stddevs)\n",
    "        return logits, stddevs\n",
    "    \n",
    "class Value(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden1, n_hidden2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_in, n_hidden1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden1, n_hidden2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden2, 1), # same as Policy, but output is only one number\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        logit = self.net(x)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e35df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Env name: InvertedPendulum-v5, \n",
      "- Observation space: Box(-inf, inf, (4,), float64) \n",
      "- Action space:      Box(-1.0, 1.0, (1,), float32) \n",
      "- Reward threshold: 950.0\n",
      "\n",
      "Env name: InvertedDoublePendulum-v5, \n",
      "- Observation space: Box(-inf, inf, (9,), float64) \n",
      "- Action space:      Box(-1.0, 1.0, (1,), float32) \n",
      "- Reward threshold: 9100.0\n",
      "\n",
      "Env name: Reacher-v5, \n",
      "- Observation space: Box(-inf, inf, (10,), float64) \n",
      "- Action space:      Box(-1.0, 1.0, (2,), float32) \n",
      "- Reward threshold: -3.75\n",
      "\n",
      "Env name: HalfCheetah-v5, \n",
      "- Observation space: Box(-inf, inf, (17,), float64) \n",
      "- Action space:      Box(-1.0, 1.0, (6,), float32) \n",
      "- Reward threshold: 4800.0\n",
      "\n",
      "Env name: Hopper-v5, \n",
      "- Observation space: Box(-inf, inf, (11,), float64) \n",
      "- Action space:      Box(-1.0, 1.0, (3,), float32) \n",
      "- Reward threshold: 3800.0\n",
      "\n",
      "Env name: Walker2d-v5, \n",
      "- Observation space: Box(-inf, inf, (17,), float64) \n",
      "- Action space:      Box(-1.0, 1.0, (6,), float32) \n",
      "- Reward threshold: None\n",
      "\n",
      "Env name: Swimmer-v5, \n",
      "- Observation space: Box(-inf, inf, (8,), float64) \n",
      "- Action space:      Box(-1.0, 1.0, (2,), float32) \n",
      "- Reward threshold: 360.0\n"
     ]
    }
   ],
   "source": [
    "envs = [\n",
    "    # \"Pendulum-v1\",\n",
    "    # \"MountainCarContinuous-v0\",\n",
    "    # \"BipedalWalker-v3\",\n",
    "    # \"LunarLander-v3\",\n",
    "    # \"CarRacing-v3\", # image input\n",
    "    \"InvertedPendulum-v5\", # OK with gamma = 0.9995, alpha_value = 1e-3, alpha = 1e-3, 1000 episodes\n",
    "    \"InvertedDoublePendulum-v5\",\n",
    "    \"Reacher-v5\",\n",
    "    \"HalfCheetah-v5\",\n",
    "    \"Hopper-v5\",\n",
    "    \"Walker2d-v5\",\n",
    "    \"Swimmer-v5\"\n",
    "]\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "for env_name in envs:\n",
    "    env = gym.make(env_name, continuous=True) if env_name in [\"LunarLander-v3\", \"CarRacing-v3\"] else gym.make(env_name)\n",
    "    wrapped_env = RescaleAction(env, min_action=-1, max_action=1)\n",
    "    wrapped_env.reset()\n",
    "    print(\"\\nEnv name: {}, \\n- Observation space: {} \\n- Action space:      {} \\n- Reward threshold: {}\".format(env_name, wrapped_env.observation_space, wrapped_env.action_space, env.spec.reward_threshold))\n",
    "    # The observation spaces are all from -inf to +inf (with various dimensions)\n",
    "\n",
    "# TODO\n",
    "# try to bring rewards for all environments on a similar scale\n",
    "# same with observations & actions, only difference should be the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68aaf7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "env_name = \"InvertedDoublePendulum-v5\"\n",
    "\n",
    "n_hidden1 = 64 \n",
    "n_hidden2 = 64\n",
    "gamma = 0.99\n",
    "\n",
    "# Sutton & Barto use a separate learning rate to update the state-value function parameters\n",
    "alpha_value = 3e-4\n",
    "alpha = 3e-4\n",
    "\n",
    "n_episodes = 20_000\n",
    "print_every_n_episodes = 500\n",
    "\n",
    "early_stopping = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341cd8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 118.83357804680574\n",
      "State-value function for initial observation in this episode: 14.551530838012695\n",
      "Average reward last 100 episodes: 78.24712491262116\n",
      "\n",
      "Episode: 1000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 137.99324974676503\n",
      "State-value function for initial observation in this episode: 23.07192039489746\n",
      "Average reward last 100 episodes: 77.95293582569428\n",
      "\n",
      "Episode: 1500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 129.0499625532247\n",
      "State-value function for initial observation in this episode: 29.62936019897461\n",
      "Average reward last 100 episodes: 85.2344724982539\n",
      "\n",
      "Episode: 2000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 135.85128145854048\n",
      "State-value function for initial observation in this episode: 35.12007141113281\n",
      "Average reward last 100 episodes: 91.02658630441452\n",
      "\n",
      "Episode: 2500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 53.87734279157247\n",
      "State-value function for initial observation in this episode: 40.138240814208984\n",
      "Average reward last 100 episodes: 103.53299576662721\n",
      "\n",
      "Episode: 3000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.74625255086454\n",
      "State-value function for initial observation in this episode: 8.77513313293457\n",
      "Average reward last 100 episodes: 21.320106391776452\n",
      "\n",
      "Episode: 3500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.90079409587657\n",
      "State-value function for initial observation in this episode: 10.102925300598145\n",
      "Average reward last 100 episodes: 21.49007483806779\n",
      "\n",
      "Episode: 4000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 25.266652629084035\n",
      "State-value function for initial observation in this episode: 9.997211456298828\n",
      "Average reward last 100 episodes: 20.553233991130156\n",
      "\n",
      "Episode: 4500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.838743862594626\n",
      "State-value function for initial observation in this episode: 8.211661338806152\n",
      "Average reward last 100 episodes: 20.408991880606006\n",
      "\n",
      "Episode: 5000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.958390558410162\n",
      "State-value function for initial observation in this episode: 10.883993148803711\n",
      "Average reward last 100 episodes: 21.489387745600972\n",
      "\n",
      "Episode: 5500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 25.292236174693326\n",
      "State-value function for initial observation in this episode: 10.239025115966797\n",
      "Average reward last 100 episodes: 20.988673665783484\n",
      "\n",
      "Episode: 6000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 25.108367641354814\n",
      "State-value function for initial observation in this episode: 9.972577095031738\n",
      "Average reward last 100 episodes: 20.82477225466786\n",
      "\n",
      "Episode: 6500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.93474759789605\n",
      "State-value function for initial observation in this episode: 8.023519515991211\n",
      "Average reward last 100 episodes: 20.572721445204458\n",
      "\n",
      "Episode: 7000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.89860012610122\n",
      "State-value function for initial observation in this episode: 7.452640533447266\n",
      "Average reward last 100 episodes: 21.33268169862572\n",
      "\n",
      "Episode: 7500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 25.321477203291106\n",
      "State-value function for initial observation in this episode: 7.698683738708496\n",
      "Average reward last 100 episodes: 20.904950447409224\n",
      "\n",
      "Episode: 8000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.95644614935911\n",
      "State-value function for initial observation in this episode: 8.494503021240234\n",
      "Average reward last 100 episodes: 21.57798114049549\n",
      "\n",
      "Episode: 8500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 25.126637731251265\n",
      "State-value function for initial observation in this episode: 8.376548767089844\n",
      "Average reward last 100 episodes: 20.825041895202702\n",
      "\n",
      "Episode: 9000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.87167078350651\n",
      "State-value function for initial observation in this episode: 10.065303802490234\n",
      "Average reward last 100 episodes: 20.645246288440916\n",
      "\n",
      "Episode: 9500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.779277018035934\n",
      "State-value function for initial observation in this episode: 10.547072410583496\n",
      "Average reward last 100 episodes: 21.74441576158932\n",
      "\n",
      "Episode: 10000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.775853682417903\n",
      "State-value function for initial observation in this episode: 8.335855484008789\n",
      "Average reward last 100 episodes: 21.407147897562524\n",
      "\n",
      "Episode: 10500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.939173281956105\n",
      "State-value function for initial observation in this episode: 8.16975212097168\n",
      "Average reward last 100 episodes: 20.40710636538003\n",
      "\n",
      "Episode: 11000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.86339171582623\n",
      "State-value function for initial observation in this episode: 7.648622512817383\n",
      "Average reward last 100 episodes: 20.80056332025671\n",
      "\n",
      "Episode: 11500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.80292002722743\n",
      "State-value function for initial observation in this episode: 10.156689643859863\n",
      "Average reward last 100 episodes: 21.070726972539802\n",
      "\n",
      "Episode: 12000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.898346440936074\n",
      "State-value function for initial observation in this episode: 10.312302589416504\n",
      "Average reward last 100 episodes: 21.304641466822225\n",
      "\n",
      "Episode: 12500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.940184579933458\n",
      "State-value function for initial observation in this episode: 8.473294258117676\n",
      "Average reward last 100 episodes: 20.564831547181814\n",
      "\n",
      "Episode: 13000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.925543661030925\n",
      "State-value function for initial observation in this episode: 8.288472175598145\n",
      "Average reward last 100 episodes: 21.401426523388892\n",
      "\n",
      "Episode: 13500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.893168638541304\n",
      "State-value function for initial observation in this episode: 8.382129669189453\n",
      "Average reward last 100 episodes: 21.72610853317821\n",
      "\n",
      "Episode: 14000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.872831275905256\n",
      "State-value function for initial observation in this episode: 10.236114501953125\n",
      "Average reward last 100 episodes: 21.38615339151342\n",
      "\n",
      "Episode: 14500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.81069845153812\n",
      "State-value function for initial observation in this episode: 8.519390106201172\n",
      "Average reward last 100 episodes: 20.894869499354364\n",
      "\n",
      "Episode: 15000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.916907240637183\n",
      "State-value function for initial observation in this episode: 10.286404609680176\n",
      "Average reward last 100 episodes: 21.489831482878294\n",
      "\n",
      "Episode: 15500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.849649341484252\n",
      "State-value function for initial observation in this episode: 7.8606181144714355\n",
      "Average reward last 100 episodes: 21.072513073341558\n",
      "\n",
      "Episode: 16000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 25.362235339476825\n",
      "State-value function for initial observation in this episode: 10.141236305236816\n",
      "Average reward last 100 episodes: 21.2290411974214\n",
      "\n",
      "Episode: 16500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 25.37187855749748\n",
      "State-value function for initial observation in this episode: 9.948062896728516\n",
      "Average reward last 100 episodes: 21.304544025015698\n",
      "\n",
      "Episode: 17000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.921526484357678\n",
      "State-value function for initial observation in this episode: 10.186769485473633\n",
      "Average reward last 100 episodes: 21.48455929799789\n",
      "\n",
      "Episode: 17500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.812552441314978\n",
      "State-value function for initial observation in this episode: 10.213064193725586\n",
      "Average reward last 100 episodes: 21.252182080979477\n",
      "\n",
      "Episode: 18000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 25.41506431497961\n",
      "State-value function for initial observation in this episode: 8.7996187210083\n",
      "Average reward last 100 episodes: 22.909446456736905\n",
      "\n",
      "Episode: 18500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.88144172685511\n",
      "State-value function for initial observation in this episode: 10.142256736755371\n",
      "Average reward last 100 episodes: 20.911011488174264\n",
      "\n",
      "Episode: 19000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.911832029429686\n",
      "State-value function for initial observation in this episode: 8.539169311523438\n",
      "Average reward last 100 episodes: 21.613563346215695\n",
      "\n",
      "Episode: 19500 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 16.88723523006383\n",
      "State-value function for initial observation in this episode: 8.659167289733887\n",
      "Average reward last 100 episodes: 21.15853471383557\n",
      "\n",
      "Episode: 20000 of 20000\n",
      "Alpha: 0.0003 Alpha_value: 0.0003 Gamma 0.99\n",
      "Total reward in this episode: 25.558731267070947\n",
      "State-value function for initial observation in this episode: 10.522777557373047\n",
      "Average reward last 100 episodes: 22.314497550472957\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Initialise the environment\n",
    "env = gym.make(env_name, continuous=True) if env_name in [\"LunarLander-v3\", \"CarRacing-v3\"] else gym.make(env_name)\n",
    "wrapped_env = RescaleAction(env, min_action=-1, max_action=1)\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=seed)\n",
    "observation = torch.tensor(observation, requires_grad=False, dtype=torch.float32)\n",
    "\n",
    "policy = Policy(n_in=env.observation_space.shape[0], n_hidden1=n_hidden1, n_hidden2=n_hidden2, n_out=env.action_space.shape[0])\n",
    "\n",
    "# The state-value function maps states to a single number. From PPO: \"We donâ€™t share parameters between the policy and value function [...]\". \n",
    "value = Value(n_in=env.observation_space.shape[0], n_hidden1=n_hidden1, n_hidden2=n_hidden2)\n",
    "\n",
    "# Sutton & Barto use separate gradient update steps with separate learning rates for the policy and the state-value function\n",
    "# Adam with default parameters instead of SGD\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=alpha, maximize=True)\n",
    "optimizer_value = torch.optim.Adam(value.parameters(), lr=alpha_value, maximize=True)\n",
    "\n",
    "\n",
    "all_episode_rewards = []\n",
    "\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    \n",
    "    rewards = [] # T rewards from 1 to T\n",
    "    observations = [observation] # T observations from 0 to T-1, the policy expects tensors as input\n",
    "    actions = [] # T actions from 0 to T-1\n",
    "\n",
    "    # roll-out of one episode following the policy\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        # probabilities for actions\n",
    "        # Change 1: The policy network outputs means and standard deviations\n",
    "        pred_means, pred_stddevs = policy(observation)\n",
    "        \n",
    "        # sample an action according to the probabilities\n",
    "        # Change 2: We use the truncated normal distribution on the continuous action space, not the categorical distribution for a discrete set of actions\n",
    "        TN = TruncatedNormal(loc=pred_means,\n",
    "                             scale=pred_stddevs,\n",
    "                             low=env.action_space.low,\n",
    "                             high=env.action_space.high,\n",
    "                             tanh_loc=False)\n",
    "        action = TN.sample()\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "        observation, reward, terminated, truncated, info = env.step(action.detach().numpy())\n",
    "        observation = torch.tensor(observation, requires_grad=False, dtype=torch.float32)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # build up one episode\n",
    "        rewards.append(reward)\n",
    "        observations.append(observation)\n",
    "        actions.append(action)\n",
    "\n",
    "        # If the episode has ended then we can reset to start a new episode\n",
    "        if done:\n",
    "            observation, info = env.reset()\n",
    "            all_episode_rewards.append(sum(rewards)) # track total reward for all episodes\n",
    "\n",
    "    \n",
    "    # policy updates using policy gradients along each step of the episode\n",
    "    # We accumulate gradients over one episode and make only one gradient step per episode\n",
    "    pseudo_losses = []\n",
    "    value_losses = []\n",
    "    for t in range(len(rewards)):\n",
    "        \n",
    "        # observation at step t during episode, and action taken\n",
    "        observation = observations[t]\n",
    "        action = actions[t] \n",
    "        \n",
    "        # discounted return starting from step t\n",
    "        G = sum(gamma**i * r for i, r in enumerate(rewards[t:]))\n",
    "        \n",
    "        # removing the baseline\n",
    "        value_logit = value(observation)\n",
    "        delta = G - value_logit\n",
    "        \n",
    "        # note that delta does not only depend on the rewards (through G), but also on the parameters of the state-value network (through the value logits)\n",
    "        # note however, that we don't take the gradient of them\n",
    "        delta = delta.detach()\n",
    "\n",
    "        # this is the distribution at the respective time-step\n",
    "        pred_means, pred_stddevs = policy(observation)\n",
    "        TN = TruncatedNormal(loc=pred_means,\n",
    "                             scale=pred_stddevs,\n",
    "                             low=env.action_space.low,\n",
    "                             high=env.action_space.high,\n",
    "                             tanh_loc=False)\n",
    "\n",
    "        # we want the updated probability of the action that was actually taken\n",
    "        # Old: use score output of policy network\n",
    "        # Change 1: now use log_prob of the distribution class\n",
    "        log_prob = TN.log_prob(action).sum() # If the action space has more than one action, the log probs sum if we assume the actions to be independent.\n",
    "\n",
    "        # We accumulate gradients over one episode and make only one gradient step per episode\n",
    "        pseudo_losses.append(gamma**t * delta * log_prob)\n",
    "        value_losses.append(delta * value_logit)\n",
    "    \n",
    "    # We accumulate gradients over one episode and make only one gradient step per episode\n",
    "    pseudo_loss = torch.stack(pseudo_losses).mean()\n",
    "    value_pseudo_loss = torch.stack(value_losses).mean()\n",
    "    \n",
    "    # We now include a gradient step (also a maximization) of the value function\n",
    "    optimizer_value.zero_grad()\n",
    "    value_pseudo_loss.backward()\n",
    "    optimizer_value.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    pseudo_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # print some statistics every other episode\n",
    "    if i_episode % print_every_n_episodes == 0:\n",
    "        print(\"\\nEpisode:\", i_episode, \"of\", n_episodes)\n",
    "        print(\"Alpha:\", alpha, \"Alpha_value:\", alpha_value, \"Gamma\", gamma)\n",
    "        print(\"Total reward in this episode:\", sum(rewards))\n",
    "        print(\"State-value function for initial observation in this episode:\", value(observations[0]).item())\n",
    "        if len(all_episode_rewards) > 100: print(\"Average reward last 100 episodes:\", sum(all_episode_rewards[-100:])/100)\n",
    "\n",
    "    # TODO early stopping needs lower average reward bound applicable for several environments. Reward scaling and max_timesteps -> gives lower bound for reward per episode?\n",
    "    # if early_stopping and i_episode > 4000 and np.mean(all_episode_rewards[-200:]) < -200:\n",
    "    #     print(\"Run terminated due to early stopping at episode\", i_episode)\n",
    "    #     break\n",
    "\n",
    "# - When a policy solved the environment for one episode:\n",
    "    if all_episode_rewards[-1] > env.spec.reward_threshold:\n",
    "        print(\"Environment was solved for one episode at episode\", i_episode)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72502c7f",
   "metadata": {},
   "source": [
    "### Save and load trained policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b09a7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Uncomment to save a trained policy:\n",
    "# pickle.dump(policy, open('policies/REINFORCE_with_baseline_InvertedDoublePendulum_visualize.pkl', 'wb'))\n",
    "\n",
    "# Uncomment to load a saved policy:\n",
    "policy = pickle.load(open('policies/REINFORCE_with_baseline_InvertedDoublePendulum_visualize.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97620443",
   "metadata": {},
   "source": [
    "### Plotting training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d462eeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.4.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_150549/2371738871.py\", line 1, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/home/adrian/anaconda3/lib/python3.11/site-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m smoothing_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# Change for different levels of smoothing\u001b[39;00m\n\u001b[1;32m      3\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_episode_rewards)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/__init__.py:129\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/rcsetup.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/colors.py:56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/scale.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[1;32m     24\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[1;32m     25\u001b[0m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/ticker.py:138\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[1;32m    140\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    142\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    143\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    151\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsinhLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/transforms.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     53\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "smoothing_interval = 100 # Change for different levels of smoothing\n",
    "a = np.array(all_episode_rewards)\n",
    "smoothed_rewards = a[(len(a)%smoothing_interval):].reshape(-1, smoothing_interval).mean(1)\n",
    "\n",
    "plt.plot(smoothed_rewards)\n",
    "xtcks = np.arange(0, len(smoothed_rewards), smoothing_interval)\n",
    "plt.xticks(xtcks,  smoothing_interval*xtcks)\n",
    "\n",
    "np.where(a > env.spec.reward_threshold) # training steps at which the policy \"solved\" the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25132b",
   "metadata": {},
   "source": [
    "## Visualize the policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0abf250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the environment\n",
    "# TODO this currently does not work for me\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    pred_means, pred_stddevs = policy(torch.tensor(observation, requires_grad=False, dtype=torch.float32))\n",
    "    action = pred_means # TODO: should sampling also be stochastic?\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action.detach().numpy())\n",
    "    \n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a27f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO one more approach: rendering an image after each step and collecting them to a video\n",
    "# https://github.com/google-deepmind/dm_control/issues/39\n",
    "from dm_control import suite\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def grabFrame(env):\n",
    "    # Get RGB rendering of env\n",
    "    rgbArr = env.physics.render(480, 600, camera_id=0)\n",
    "    # Convert to BGR for use with OpenCV\n",
    "    return cv2.cvtColor(rgbArr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Load task:\n",
    "env = suite.load(domain_name=\"cartpole\", task_name=\"swingup\")\n",
    "\n",
    "# Setup video writer - mp4 at 30 fps\n",
    "video_name = 'video.mp4'\n",
    "frame = grabFrame(env)\n",
    "height, width, layers = frame.shape\n",
    "video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 30.0, (width, height))\n",
    "\n",
    "# First pass - Step through an episode and capture each frame\n",
    "action_spec = env.action_spec()\n",
    "time_step = env.reset()\n",
    "while not time_step.last():\n",
    "    action = np.random.uniform(action_spec.minimum,\n",
    "                               action_spec.maximum,\n",
    "                               size=action_spec.shape)\n",
    "    time_step = env.step(action)\n",
    "    frame = grabFrame(env)\n",
    "    # Render env output to video\n",
    "    video.write(grabFrame(env))\n",
    "\n",
    "# End render to video file\n",
    "video.release()\n",
    "\n",
    "# Second pass - Playback\n",
    "cap = cv2.VideoCapture(video_name)\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Playback', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "\n",
    "# Exit\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
